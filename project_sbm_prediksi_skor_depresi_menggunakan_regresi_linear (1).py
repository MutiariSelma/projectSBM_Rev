# -*- coding: utf-8 -*-
"""Project SBM - Prediksi Skor Depresi Menggunakan Regresi Linear

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11RiW1YO_-LRNKmjaykue6GPiuFKlEDS4
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score, KFold

from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

from google.colab import files
uploaded = files.upload()

df = pd.read_csv('MentalHealthSurvey.csv')

print(df.info())
print(df.isnull().sum())
df.columns = df.columns.str.strip()

sleep_map = {'2-4 hrs': 0, '4-6 hrs': 1, '7-8 hrs': 2}
sports_map = {'No Sports': 0, '1-3 times': 1, '4-6 times': 2, '7+ times': 3}

df['avg_sleep'] = df['average_sleep'].map(sleep_map)
df['sports'] = df['sports_engagement'].map(sports_map)

df.to_csv("MentalHealthSurvey_Preprocessed.csv", index=False)

from google.colab import files
files.download("MentalHealthSurvey_Preprocessed.csv")

df_numerik = df.select_dtypes(include=['int64', 'float64'])
correlation = df_numerik.corr()['depression'].sort_values(ascending=False).round(2)
print(correlation)
print ()

plt.figure(figsize=(10, 8))
sns.heatmap(df_numerik.corr(), annot=True, cmap='coolwarm')
plt.title("Heatmap Korelasi")
plt.show()

# Split fitur
features_main = ['academic_workload', 'academic_pressure', 'future_insecurity',
                 'financial_concerns', 'social_relationships', 'isolation']
features_extended = features_main + ['avg_sleep', 'sports']

# Target
y = df['depression']

# Linear Regression Basic
X_basic = df[features_main]
X_train_basic, X_test_basic, y_train, y_test = train_test_split(X_basic, y, test_size=0.2, random_state=42)

# Linear Regression Extended + Model Non-Linear
X_extended = df[features_extended]
X_train_ext, X_test_ext, y_train_ext, y_test_ext = train_test_split(X_extended, y, test_size=0.2, random_state=42)

# Scaling Basic
scaler_basic = StandardScaler()
X_train_basic_scaled = scaler_basic.fit_transform(X_train_basic)
X_test_basic_scaled = scaler_basic.transform(X_test_basic)

# Scaling Extended
scaler_ext = StandardScaler()
X_train_ext_scaled = scaler_ext.fit_transform(X_train_ext)
X_test_ext_scaled = scaler_ext.transform(X_test_ext)

# Linear Regression Basic
lr_basic = LinearRegression()
lr_basic.fit(X_train_basic_scaled, y_train)
y_pred_lr_basic = lr_basic.predict(X_test_basic_scaled)

# Linear Regression Extended
lr_ext = LinearRegression()
lr_ext.fit(X_train_ext_scaled, y_train_ext)
y_pred_lr_ext = lr_ext.predict(X_test_ext_scaled)

# Random Forest (Default)
rf = RandomForestRegressor(random_state=42)
rf.fit(X_train_ext, y_train_ext)
y_pred_rf = rf.predict(X_test_ext)

# XGBoost (Default)
xgb_default = XGBRegressor(random_state=42, verbosity=0)
xgb_default.fit(X_train_ext, y_train_ext)
y_pred_xgb = xgb_default.predict(X_test_ext)

# LightGBM (Default)
lgbm_default = LGBMRegressor(random_state=42, verbose=-1)
lgbm_default.fit(X_train_ext, y_train_ext)
y_pred_lgbm = lgbm_default.predict(X_test_ext)

# Random Forest (Tuned)
param_grid = {
    'n_estimators': [10, 50, 100],
    'max_depth': [None, 5, 10],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'max_features': ['sqrt', 'log2', None]
}

rf_tune = RandomForestRegressor(random_state=42)
grid_search = GridSearchCV(estimator=rf_tune, param_grid=param_grid,
                           cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')

grid_search.fit(X_train_ext, y_train_ext)

print("Best parameters:", grid_search.best_params_)
print("Best MAE:", -grid_search.best_score_)
print()

best_rf = grid_search.best_estimator_
y_pred_best_rf = best_rf.predict(X_test_ext)


# XGBoost (Tuned)
param_grid_xgb = {
    'n_estimators': [50, 100, 150],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

xgb_tune = XGBRegressor(random_state=42, verbosity=0)
grid_search_xgb = GridSearchCV(estimator=xgb_tune, param_grid=param_grid_xgb,
                               cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')

grid_search_xgb.fit(X_train_ext, y_train_ext)

print("Best parameters (XGBoost):", grid_search_xgb.best_params_)
print("Best MAE (XGBoost):", -grid_search_xgb.best_score_)
print()

best_xgb = grid_search_xgb.best_estimator_
y_pred_best_xgb = best_xgb.predict(X_test_ext)

# LightGBM Tuned
param_grid_lgbm = {
    'n_estimators': [50, 100, 150],
    'max_depth': [3, 5, 7, -1],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

lgbm_tune = LGBMRegressor(random_state=42, verbose=-1)
grid_search_lgbm = GridSearchCV(estimator=lgbm_tune, param_grid=param_grid_lgbm,
                                cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')

grid_search_lgbm.fit(X_train_ext, y_train_ext)

print("Best parameters (LightGBM):", grid_search_lgbm.best_params_)
print("Best MAE (LightGBM):", -grid_search_lgbm.best_score_)
print()

best_lgbm = grid_search_lgbm.best_estimator_
y_pred_best_lgbm = best_lgbm.predict(X_test_ext)

# Evaluate Function
def evaluate_model(y_true, y_pred, name):
    print(f"{name} Evaluation:")
    print("MAE:", round(mean_absolute_error(y_true, y_pred), 2))
    print("RMSE:", round(np.sqrt(mean_squared_error(y_true, y_pred)), 2))
    print("R² Score:", round(r2_score(y_true, y_pred), 2))
    print()

# Evaluate All Models
evaluate_model(y_test, y_pred_lr_basic, "Linear Regression (Basic)")
evaluate_model(y_test_ext, y_pred_lr_ext, "Linear Regression (Extended)")
evaluate_model(y_test_ext, y_pred_rf, "Random Forest (Default)")
evaluate_model(y_test_ext, y_pred_best_rf, "Random Forest (Tuned)")
evaluate_model(y_test_ext, y_pred_xgb, "XGBoost (Default)")
evaluate_model(y_test_ext, y_pred_best_xgb, "XGBoost (Tuned)")
evaluate_model(y_test_ext, y_pred_lgbm, "LightGBM (Default)")
evaluate_model(y_test_ext, y_pred_best_lgbm, "LightGBM (Tuned)")

# Feature Importance Plot
def plot_feature_importance(model, feature_names, model_name):
    importances = model.feature_importances_
    indices = np.argsort(importances)[::-1]

    plt.figure(figsize=(8, 6))
    plt.title(f"Feature Importance - {model_name}")
    sns.barplot(x=importances[indices], y=np.array(feature_names)[indices])
    plt.show()
    print ()

# Plot untuk RF (Tuned) dan XGBoost
plot_feature_importance(best_rf, features_extended, "Random Forest (Tuned)")
#plot_feature_importance(xgb, features_extended, "XGBoost")

# Plot Feature Importance - XGBoost Tuned
plot_feature_importance(best_xgb, features_extended, "XGBoost (Tuned)")

# Plot Feature Importance - LightGBM Tuned
plot_feature_importance(best_lgbm, features_extended, "LightGBM (Tuned)")

# Scatter Plot Prediksi
plt.figure(figsize=(10, 6))
plt.scatter(y_test_ext, y_pred_best_rf, alpha=0.6, label='Random Forest (Tuned)', color='green')
plt.scatter(y_test_ext, y_pred_xgb, alpha=0.6, label='XGBoost', color='blue')
plt.scatter(y_test, y_pred_lr_basic, alpha=0.6, label='Linear Regression (Basic)', color='red')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)

plt.xlabel("Actual Depression Score")
plt.ylabel("Predicted Depression Score")
plt.title("Model Predictions vs Actual (Test Set)")
plt.legend()
plt.grid(True)
plt.show()

def cross_validate_model(model, X, y, model_name, cv=5):
    kf = KFold(n_splits=cv, shuffle=True, random_state=42)

    mae_scores = -cross_val_score(model, X, y, cv=kf, scoring='neg_mean_absolute_error')
    rmse_scores = np.sqrt(-cross_val_score(model, X, y, cv=kf, scoring='neg_mean_squared_error'))
    r2_scores = cross_val_score(model, X, y, cv=kf, scoring='r2')

    print(f"{model_name} Cross-Validation Results ({cv}-fold):")
    print(f"MAE: Mean = {mae_scores.mean():.2f}, Std = {mae_scores.std():.2f}")
    print(f"RMSE: Mean = {rmse_scores.mean():.2f}, Std = {rmse_scores.std():.2f}")
    print(f"R²: Mean = {r2_scores.mean():.2f}, Std = {r2_scores.std():.2f}")
    print()

# Linear Regression Basic
cross_validate_model(LinearRegression(), X_basic, y, "Linear Regression (Basic)", cv=5)

# Linear Regression Extended
cross_validate_model(LinearRegression(), X_extended, y, "Linear Regression (Extended)", cv=5)

# Random Forest (Tuned) → pakai param best_rf
cross_validate_model(RandomForestRegressor(**best_rf.get_params()), X_extended, y, "Random Forest (Tuned)", cv=5)

# XGBoost (Tuned)
cross_validate_model(XGBRegressor(**best_xgb.get_params()), X_extended, y, "XGBoost (Tuned)", cv=5)

# LightGBM (Tuned)
cross_validate_model(LGBMRegressor(**best_lgbm.get_params()), X_extended, y, "LightGBM (Tuned)", cv=5)